---
title: "[ISLR] Chapter 04 - Classification"
mathjax: true
comments: true
---

# 4.1 An Overview of Classification
Predicting a qualitative response for an observation can be referred to as classifying that observation.
This chapter discusses three classifiers- logistic regression, linear discriminant analysis, K-nearest neighbors.
# 4.2 Why Not Linear Regression?
Linear regression for classification on two class produces good result but isn't preferable for more classes.
# 4.3 Logistic Regression
Logistic regression models the *probability* that Y belongs to a particular category. For example, to classify load defaulters (yes or no) given their balance, probability of default given balance will be- 
$$
p(X) = \beta_{0} + \beta_{1}X \tag{1}
$$
## 4.3.1 The Logistic Model
This is for a two class problem. When predicting a response, the linear model sometimes produces values greater than 1 or smaller than 0. This is unwarranted because the probability of something must be in [0,1] range. So the **logistic function** is-
$$
p(X) = \frac{e^{\beta_{0} + \beta_{1}X}}{1+e^{\beta_{0} + \beta_{1}X}} \tag{2}
$$
After a bit of manipulation of (2)- 
$$
\frac{p(X)}{1-p(X)}=e^{\beta_{0} + \beta_{1}X} 
$$
This left hand side is called the odds, range is [0, $\infty$) which respectively indicates very high probability or low probability of something. For example, on average 1 in 5 people with an odds of 1/4 will default if $p(X) = 0.2$.
## 4.3.2 Estimating the Regression Coefficients
The coeffs $\beta_{0}, \beta_{1}$ are estimated using *maximum likelihood*. Intuition behind maximum likelihood is that we seek $\beta_{0}, \beta_{1}$ such that the predicted probability $\hat{p}(X)$ yields a number close to 1 for all in class 1 and number close to 0 for all in class 2. The mathematical details of maximum likelihood is beyond the scope of this book. Any statistical package/software can be used to estimate the coefficients. 
## 4.3.3 Making Predictions
After estimating the coefficients, just plug in them into (2) for prediction. Qualitative predictors can also be used in form of dummy variable approach. For example, the [Default](https://rdrr.io/cran/ISLR/man/Default.html) data set contains qualitative variable *student*. To fit the model, 1 for student, 0 for non-student is used. So, if the coeffs are -3.5041(intercept) and 0.4049(student), the prediction is,
$$
\hat{Pr}(default=Yes|student=Yes) = \frac{e^{-3.5041+0.4049\times1}}{1+e^{-3.5041+0.4049\times1}} \\
\hat{Pr}(default=Yes|student=Yes) = \frac{e^{-3.5041+0.4049\times0}}{1+e^{-3.5041+0.4049\times0}}
$$
## 4.3.4 Multiple Logistic Regression
