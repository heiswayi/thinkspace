# Data
Bangla Newspaper Dataset. Contains 400K+ news articles with category both in Bengali and English, multilabel tags, titles etc.  Data collected from https://www.prothomalo.com/archive [Copyright owned by Prothom Alo]
# Preprocessing
## Cleaning
Each article contains stopwords, urls, and other unnecessary and uninformative words and characters. This step cleans the articles.
```
Algorithm 1 News Article Preprocess
Require: News article list "articles"
Ensure: Filtered articles
-------------------------------------------------
punctuations = [!”#$%&()*+,-./:;<=>?@[\]^_`{|}~,।‘’-]
results = ""
for article in articles do
    for word in article do
        remove punctuations from word
        if word is a stopword do
            continue
        else do
            results.append(word)
```
## Tokenization
Given a character or word sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called token. It is a useful unit for processing. Each article is cosidered a document, and all sentences were splitted on whitespace, thus creating single tokens.
## Vectorization
This is the process of turing each text into a sequence of integers where each integer is the index of a token in a dictionary. Meaning, all the unique words in the dataset creates a dictionary, then all words are replaced by corresponding index of that in the dictionary. 
## Padding
Articles maybe of different lengths, but neurons in input layer has to be fixed. In this step, we padded all articles to have the same length = 200. If a article is more than 200 words, it is cut at the end. If it's less than 200, 0 added in the beginning. \
After these steps, we end up with a $$M x 200$$ matrix, here $$M$$ is the total number of articles. 

## Output Selection
In the dataset, each article is assigned to one or more tags. We have tried to solve tag suggestion as a supervised problem, therefore it is necessary to pick a number of predetermined tags and use a classifier to label new articles. In the dataset, a total of 3084 unique tags were used. Most common ones are বিশাল বাংলা,অপরাধ, রাজনীতি, চট্টগ্রাম বিভাগ, খেলা, ঢাকা বিভাগ, আইন ও বিচার. We have selected 100 most frequent tags as candidate. They are 'বিশাল বাংলা', 'অপরাধ', 'রাজনীতি', 'চট্টগ্রাম বিভাগ', 'খেলা', 'ঢাকা বিভাগ', 'আইন ও বিচার', 'রাজধানী (জাতীয়)', 'চট্টগ্রাম', 'রাজশাহী বিভাগ', 'আন্তর্জাতিক ক্রিকেট', 'সরকার', 'মহানগর', 'দুর্ঘটনা', 'ভারত', 'আন্তর্জাতিক ফুটবল', 'মতামত', 'বিনোদন', 'রংপুর বিভাগ', 'বাণিজ্য', 'আমার চট্টগ্রাম', 'খুলনা বিভাগ', 'খবর', 'ক্রিকেট', 'রাজধানী', 'পড়াশোনা', 'বলিউড', 'সিলেট বিভাগ', 'বরিশাল বিভাগ', 'লেখকের কলাম', 'ফুটবল', 'এশিয়া', 'খবরাখবর', 'যুক্তরাষ্ট্র', 'সিলেট', 'বগুড়া', 'নারায়ণগঞ্জ', 'রাজশাহী', 'টেলিভিশন', 'নির্বাচন', 'বিজ্ঞান-প্রযুক্তি', 'চলচ্চিত্র', 'সম্পাদকীয়:', 'দেশের ক্রিকেট', 'কক্সবাজার', 'ঢাকা', 'গাজীপুর', 'আরব বিশ্ব', 'আনন্দ', 'বিএনপি', 'আওয়ামী লীগ', 'ইউরোপ', 'রংপুর', 'রস+আলো', 'অধুনা', 'নকশা', 'একাদশ সংসদ নির্বাচন', 'ছুটির দিনে', 'কুমিল্লা', 'ফরিদপুর', 'ময়মনসিংহ', 'পরিবেশ', 'বাণিজ্য সংবাদ', 'দেশের ফুটবল', 'হলিউড', 'বরিশাল', 'প্রতিষ্ঠানের খবর', 'পাকিস্তান', 'সিরাজগঞ্জ', 'ব্রাহ্মণবাড়িয়া', 'পরামর্শ', 'আন্তর্জাতিক', 'পৌরসভা নির্বাচন', 'শেয়ারবাজার', 'মৌলভীবাজার', 'চিঠিপত্র', 'দিনাজপুর', 'খুলনা', 'দূর পরবাস', 'স্বপ্ন নিয়ে', 'যশোর', 'নীলফামারী', 'শিল্প ও সাহিত্য', 'রাজবাড়ী', 'পাবনা', 'কুষ্টিয়া', 'সংস্কৃতি', 'দূর্ঘটনা', 'বাংলা গান', 'নাটোর', 'টাঙ্গাইল', 'যুক্তরাজ্য', 'আমার ডাক্তার', 'সুনামগঞ্জ', 'পটুয়াখালী', 'ফেনী', 'মোবাইল ফোন', 'জেনে নিন', 'জীবনধারা', and 'সংগীত'. Less frequent tags were not selected because for them we do not have enough training data. After selection of tags, we removed any article with tag(s) other than any of these 100. We were finally left with M = 3256007 news articles. 
## Label Encoding
In this step, all the selected tags are encoded to use as prediction. Since each article can be assigned with one or more tags, any output representation must have all the tags in it. For example, if total three tags- 'বিশাল বাংলা', 'অপরাধ', 'রাজনীতি' are used in the dataset, any article in that set will need three binary number to represent its tags. [1,0,0] means its tag is - only 'বিশাল বাংলা'. [1,0,1]- means its tags are- 'বিশাল বাংলা' and 'রাজনীতি'. So in out dataset, the output dimension will be $$M x 100$$.
## Dataset Bias
The probability of a tag being predicted increases or decreases with number of article containing it in the training set. Meaning, if a tag is assigned to more articles than other tags, then it will be predicted more frequently. This is called dataset bias, and our dataset suffers from it. There are many methods to overcome this, such as undersampling, oversampling, class weights. We have used class weights in our model. Class weights is a vector of weights that has one to one mapping to the set of unique tags. Meaning for each tag, we have a weight. These weights are used in training, as a reward to the model if it predicts less frequent tags. 
```
Algorithm 2 Generating Class Weights for N Classes
Require: t = list of tags,
        freq = list of number of articles containing each tags,
        N = number of total article in the dataset
Return: w = list of weights
-------------------------------------------
for tag in t do
    w(t) <- N / freq(t)
```
## Word Embedding
Words themselves or frequency of any words do not hold any special meaning to neural networks. So as input layer, just plain tokenized articles are not that useful. Word Embedding is a good alternative. It represents words based on similarity with other words. We used word2vec model to generate a Word Embedding for this dataset. Each input words are embedding vectors of length 50. That does not mean that one hot encoding is unnecessary. One hot encoding is used to obtain Word Embedding, then Word Embedding is used as input to the network.
# Experiment
Recurrent Neural Network, more specifically Long Short Term Memory with 1 hidden layer is used as classifier. Here are different setups.
## Number of Class = 50
Training data: X = (227924,200), y = (227924,50) \
Test data: X = (97683,200,50), y = (97683,50) \
Model: 
- Input Layer: (227924,200,50) [50 is the embedding vector length]
- LSTM Layer: (128), dropout = 0.2, recurrent 0.2 
- Output Layer: (227924,50), activation= sigmoid 

Result: 
- Training Loss =  2.1577
- Training Categorical Crossentropy = 3.2131
- Test Loss = 0.0700
- Test Categorical Crossentropy = 3.127 
